---
title: "Selected Exercises from Chapter 5"
output: github_document
---

```{r echo = FALSE}
library(Cairo)
knitr::opts_chunk$set(
  fig.path = "figs/ch05ex_figs/ch05ex-"
)
```

Libraries and helper functions we'll need:

```{r warning=FALSE, message=FALSE}
library(brms)
library(coda)
library(ggplot2)
library(RColorBrewer)

# The following helper functions are from the 'rethinking' package.
# https://github.com/rmcelreath/rethinking

col.alpha <- function( acol , alpha=0.2 ) {
    acol <- col2rgb(acol)
    acol <- rgb(acol[1]/255,acol[2]/255,acol[3]/255,alpha)
    acol
}

col.desat <- function( acol , amt=0.5 ) {
    acol <- col2rgb(acol)
    ahsv <- rgb2hsv(acol)
    ahsv[2] <- ahsv[2] * amt
    hsv( ahsv[1] , ahsv[2] , ahsv[3] )
}

rangi2 <- col.desat("blue", 0.5)
```

# 5.13

Here is the complete dataset from Table 3.3 in the book. A `street_type` of `1` corresponds to "residential" streets, `2` corresponds to "fairly busy" streets, and `3` corresponds to "busy" streets.

```{r}
bike_data <- data.frame(
    bikes = as.integer(c(16, 9, 10, 13, 19, 20, 18, 17, 35, 55, 12, 1, 2, 4, 9, 7, 9, 8, 8, 35, 31, 19,
                         38, 47, 44, 44, 29, 18, 10, 43, 5, 14, 58, 15, 0, 47, 51, 32, 60, 51, 58, 59,
                         53, 68, 68, 60, 71, 63, 8, 9, 6, 9, 19, 61, 31, 75, 14, 25)),
    other = as.integer(c(58, 90, 48, 57, 103, 57, 86, 112, 273, 64, 113, 18, 14, 44, 208, 67, 29, 154,
                         29, 415, 425, 42, 180, 675, 620, 437, 47, 462, 557, 1258, 499, 601, 1163, 700,
                         90, 1093, 1459, 1086, 1545, 1499, 1598, 503, 407, 1494, 1558, 1706, 476, 752,
                         1248, 1246, 1596, 1765, 1290, 2498, 2346, 3101, 1918, 2318)),
    street_type = as.factor(c(rep(1, 18), rep(2, 20), rep(3, 20))),
    bike_route = as.integer(c(rep(1, 10), rep(0, 8), rep(1, 10), rep(0, 10), rep(1, 10), rep(0, 10)))
)
bike_data$total <- bike_data$bikes + bike_data$other
bike_data$block_id <- 1:nrow(bike_data)
bike_data
```

I'd like to incorporate the full dataset into a model instead of restricting my attention to just the observations on residential bike routes as the book suggests. After controlling for the main and interacting effects of street type and bike route, all of the observations can share information.

```{r warning=FALSE, message=FALSE, results=FALSE}
m5e_1 <- brm(
    bikes | trials(total) ~ (1 | block_id) + street_type*bike_route,
    family = binomial,
    data = bike_data,
    iter = 3e4,
    warmup = 2e3,
    chains = 4,
    cores = 4
)
```

```{r}
summary(m5e_1)
```

The positive coefficients on the interaction terms tell us that the proportions of bike traffic on fairly busy and busy streets are more sensitive to the presence of a bike route than the proportions on residential streets (after controlling for the main effects of street type). I'm not sure what constitutes a "bike route" in this data, but this could reflect cyclists avoiding streets where they are likely to be forced to interact with motor vehicles.

## 5.13 (c)

Below we'll make a series of plots comparing the raw proportions, drawn as circles whose area is proportional to the sample size, to the adjusted estimates, drawn as solid blue dots. We'll also draw 89% probability intervals as blue lines through each estimate.

```{r dev="CairoPNG"}
samples <- as.data.frame(m5e_1)
logit_block <- sapply(1:58, function(i) samples$b_Intercept + samples[,7 + i])

logit_residential <- sapply(
    1:18,
    function(i) logit_block[,i] + (if(i <= 10) samples$b_bike_route else 0)
)
residential <- inv_logit_scaled(logit_residential)
residential_mu <- apply(residential, 2, mean)
residential_PI <- apply(residential, 2, function(x) HPDinterval(as.mcmc(x), prob = 0.89)[1,])
raw_residential <- bike_data$bikes[1:18]/bike_data$total[1:18]

ymax = 0.51
plot(
    1:18, raw_residential,
    cex = 0.25*sqrt(bike_data$total[1:18]),
    ylim = c(0, ymax),
    xlab = "block number", ylab = "proportion of bikes in traffic",
    main = "Raw proportions versus adjusted estimates - residential"
)
points(1:18, residential_mu, pch = 16, col = rangi2)
for(i in 1:18)
    lines(c(i, i), residential_PI[,i], col = rangi2)
lines(c(10.5, 10.5), c(-0.02, ymax + 0.02), lty = 2)
text(5.25, ymax - 0.02, "bike lane")
text(14.7, ymax - 0.02, "no bike lane")
```

```{r dev="CairoPNG"}
logit_fairlybusy <- sapply(
    19:38,
    function(i) logit_block[,i] + samples$b_street_type2 +
        (if(i <= 28) samples$b_bike_route + samples$b_street_type2.bike_route else 0)
)
fairlybusy <- inv_logit_scaled(logit_fairlybusy)
fairlybusy_mu <- apply(fairlybusy, 2, mean)
fairlybusy_PI <- apply(fairlybusy, 2, function(x) HPDinterval(as.mcmc(x), prob = 0.89)[1,])
raw_fairlybusy <- bike_data$bikes[19:38]/bike_data$total[19:38]

plot(
    1:20, raw_fairlybusy,
    cex = 0.25*sqrt(bike_data$total[19:38]),
    ylim = c(0, ymax),
    xlab = "block number", ylab = "proportion of bikes in traffic",
    main = "Raw proportions versus adjusted estimates - fairly busy"
)
points(1:20, fairlybusy_mu, pch = 16, col = rangi2)
for(i in 1:20)
    lines(c(i, i), fairlybusy_PI[,i], col = rangi2)
lines(c(10.5, 10.5), c(-0.02, ymax + 0.02), lty = 2)
text(5.25, ymax - 0.02, "bike lane")
text(15.75, ymax - 0.02, "no bike lane")
```

```{r dev="CairoPNG"}
logit_busy <- sapply(
    39:58,
    function(i) logit_block[,i] + samples$b_street_type3 +
        (if(i <= 48) samples$b_bike_route + samples$b_street_type3.bike_route else 0)
)
busy <- inv_logit_scaled(logit_busy)
busy_mu <- apply(busy, 2, mean)
busy_PI <- apply(busy, 2, function(x) HPDinterval(as.mcmc(x), prob = 0.89)[1,])
raw_busy <- bike_data$bikes[39:58]/bike_data$total[39:58]

plot(
    1:20, raw_busy,
    cex = 0.25*sqrt(bike_data$total[39:58]),
    ylim = c(0, ymax),
    xlab = "block number", ylab = "proportion of bikes in traffic",
    main = "Raw proportions versus adjusted estimates - busy"
)
points(1:20, busy_mu, pch = 16, col = rangi2)
for(i in 1:20)
    lines(c(i, i), busy_PI[,i], col = rangi2)
lines(c(10.5, 10.5), c(-0.02, ymax + 0.02), lty = 2)
text(5.25, ymax - 0.02, "bike lane")
text(15.75, ymax - 0.02, "no bike lane")
```

We can see in these plots that the estimates are pulled toward each other, and that those corresponding to smaller sample sizes are pulled more. Let's plot all three of these on the same axis to summarize.

```{r dev="CairoPNG", fig.width=9}
plot(
    1:58, c(raw_residential, raw_fairlybusy, raw_busy),
    cex = 1.2,
    ylim = c(0, 0.47),
    xlab = "block number", ylab = "proportion of bikes in traffic",
    main = "Raw proportions versus adjusted estimates",
)
points(1:58, c(residential_mu, fairlybusy_mu, busy_mu), pch = 16, col = rangi2)
lines(c(18.5, 18.5), c(-0.02, 0.49))
lines(c(38.5, 38.5), c(-0.02, 0.49))
lines(c(10.5, 10.5), c(-0.02, 0.49), lty = 2, col = col.alpha("black", 0.4))
lines(c(28.5, 28.5), c(-0.02, 0.49), lty = 2, col = col.alpha("black", 0.4))
lines(c(48.5, 48.5), c(-0.02, 0.49), lty = 2, col = col.alpha("black", 0.4))
text(5, 0.46, "residential")
text(28.5, 0.46, "fairly busy")
text(48.5, 0.46, "busy")
```

Note that we've omitted the probability intervals and the information on sample sizes.

## 5.13 (d)

The way that the model is parameterized, the intercept corresponds to the average proportion of bicycles on residential streets that aren't labeled bike routes. So we just need to add the main coefficient on `bike_route` to get the average proportion on residential streets labeled bike routes.

```{r dev="CairoPNG"}
logit_avg_prop <- samples$b_Intercept + samples$b_bike_route  # residential & bike route
avg_prop <- inv_logit_scaled(logit_avg_prop)
avg_prop_mu <- mean(avg_prop)
avg_prop_PI <- HPDinterval(as.mcmc(avg_prop))[1,]

plot(
    density(avg_prop),
    xlab = "average proportion of bicycles", ylab = "density",
    main = "Posterior for the underlying average proportion of bicycles"
)

points(avg_prop_mu, 0.2, col = rangi2, pch = 16)

lines(avg_prop_PI, c(0.2, 0.2), lwd = 2, col = rangi2)
```

```{r}
s <- data.frame(
    mean = round(avg_prop_mu, 3),
    sd = round(sd(avg_prop), 3),
    HPDI_95 = paste(round(avg_prop_PI[1], 2), "to", round(avg_prop_PI[2], 2))
)
rownames(s) <- c("average proportion of bicycles")
s
```

## 5.13 (e)

We calculate the posterior predictive distribution **p** for the proportion of bicycles on a residential street labeled a bike route by averaging over the hyperparameters of the adaptive normal prior. The predictive distribution for the number of bicycles is then the averaged binomial distribution

**predictive(k | n = 100) = E[Binomial(k | n = 100, p)].**

```{r dev="CairoPNG"}
logit_p <- rnorm(
    nrow(samples),
    mean = logit_avg_prop,
    sd = samples$sd_block_id__Intercept
)
p <- inv_logit_scaled(logit_p)

predictive <- rbinom(nrow(samples), size = 100, prob = p)

plot(
    table(predictive)/length(predictive),
    xlab = "bicycles", ylab = "probability",
    main = "Posterior predictive distribution for the number of bicycles"
)
```

```{r}
predictive_PI <- HPDinterval(as.mcmc(predictive))[1,]
s <- data.frame(
    median = median(predictive),
    sd = round(sd(predictive), 1),
    HPDI_95 = paste(predictive_PI[1], "to", predictive_PI[2])
)
rownames(s) <- c("new city block prediction")
s
```

I suppose I would trust this interval less if the conditions under which the new street is observed (e.g. time of day, day of week, season) differ from those under which the data were observed.

# 5.15

The data for this example is available from [Gelman's website](http://www.stat.columbia.edu/~gelman/book/data/).

```{r}
metatext <- readChar("meta.asc", file.info("meta.asc")$size)
# skip the preamble
metatext <- substr(metatext, regexpr("\n\n", metatext) + 2, nchar(metatext))
# remove leading and trailing spaces around newlines
metatext <- gsub("\n ", "\n", metatext)
metatext <- gsub(" \n", "\n", metatext)
# replace repeated spaces with tabs
metatext <- gsub(" +", "\t", metatext)
# read the tab-delimited data into a data frame
meta <- read.delim(text = metatext)
```

We formulated the following bivariate binomial model for this data [in our notes on this chapter](https://github.com/szego/BDA-singalong/blob/master/ch05.md#a-multivariate-binomial-model).

```{r warning=FALSE, message=FALSE, results=FALSE}
f1 <- bf(cdeaths | trials(ctotal) ~ (1 |p| study))
f2 <- bf(tdeaths | trials(ttotal) ~ (1 |p| study))

m5e_2 <- brm(
    mvbf(f1, f2),
    family = binomial,
    data = list(
        cdeaths = meta$control.deaths,
        ctotal = meta$control.total,
        tdeaths = meta$treated.deaths,
        ttotal = meta$treated.total,
        study = meta$study
    ),
    iter = 1e5,
    warmup = 3e3,
    chains = 4,
    cores = 4,
    control = list(adapt_delta = 0.95)
)
```

```{r}
summary(m5e_2)
```

We'll just focus on part (e) of this exercise.

## 5.15 (e)

The strength of our model is that we can simulate the control and treated deaths directly. First we sample from the bivariate normal distribution of logit-probabilities.

```{r}
samples <- as.data.frame(m5e_2)
logit_probs <- sapply(
    1:nrow(samples),
    function(i) {
        Mu <- c(samples$b_cdeaths_Intercept[i], samples$b_tdeaths_Intercept[i])
        DS <- diag(c(samples$sd_study__cdeaths_Intercept[i], samples$sd_study__tdeaths_Intercept[i]))
        Rho <- matrix(c(1, rep(samples$cor_study__cdeaths_Intercept__tdeaths_Intercept[i], 2), 1), nrow = 2)
        Sigma <- DS %*% Rho %*% DS
        MASS::mvrnorm(1, Mu, Sigma)
    }
)
probs <- inv_logit_scaled(logit_probs)
```

Then we use these probabilities to simulate the binomial outcomes in control groups and treated groups with 100 patients each.

```{r dev="CairoPNG", fig.width=9, fig.height=8}
control <- rbinom(nrow(samples), size = 100, prob = probs[1,])
treated <- rbinom(nrow(samples), size = 100, prob = probs[2,])

df <- data.frame(x = control, y = treated)
rf <- colorRampPalette(rev(brewer.pal(11, "Spectral")))
r <- rf(32)
ggplot(df, aes(x,y)) +
    stat_bin2d(binwidth = 1, origin = -0.5, aes(fill = ..density..)) +
    scale_fill_gradientn(name='density', colors = r) +
    geom_abline(intercept = 0, slope = 1, color = "white") +
    labs(
        x = "deaths in control group",
        y = "deaths in treated group",
        title = "Predictive distribution for the numbers of deaths in the control and treated groups"
    ) +
    coord_fixed(xlim = c(0, 60), ylim = c(0, 60))
```

Here the white diagonal line corresponds to equal numbers of deaths in both groups.

The estimated probability that more patients will die in the treated group than in the control group is

```{r}
sum(treated > control)/length(treated)
```

Note that this is different from *the probability that the chance of dying in the treated group is higher than the chance of dying in the control group*, which is

```{r}
sum(probs[2,] > probs[1,])/length(probs[2,])
```

If this seems strange, consider an example where the chance of dying in the treated group (0.09, say) is surely lower than the chance of dying in the control group (0.11, say). There's still a non-negligible probability that more patients die in the treated group:

```{r}
control_fake <- rbinom(1e6, size = 100, prob = 0.11)
treated_fake <- rbinom(1e6, size = 100, prob = 0.09)
sum(treated_fake > control_fake)/length(treated_fake)
```

***

[Antonio R. Vargas](https://github.com/szego)

9 Dec 2018





































